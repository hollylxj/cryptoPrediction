{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from math import pi\n",
    "import time\n",
    "import random\n",
    "from operator import add\n",
    "import pickle\n",
    "import pandas\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "#import ql\n",
    "import math\n",
    "from qla import QLearningApproxAlgorithm\n",
    "\n",
    "Inf = math.inf\n",
    "\n",
    "with open('../BTC-USD-60.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "#:chunk = filter_df(df_chunk, event_type='Fill')\n",
    "# sort the data based on ascending-order of time\n",
    "data = data.sort_values(by=['time'])\n",
    "price = np.array(data.close)\n",
    "time = np.array(data.time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def switch(price,buckets):# decide which state the price belongs to\n",
    "    temp = [0]*len(buckets)    \n",
    "    for i in range(len(buckets)-1):\n",
    "        if price >= buckets[i] and price < buckets[i+1]:\n",
    "            return i\n",
    "\n",
    "def transition(state, action, price, time):\n",
    "    newState = state.clone(state.marketPrice, time)\n",
    "    if action < 0:\n",
    "        newState.sellCoin(-action)\n",
    "    elif action > 0:\n",
    "        newState.buyCoin(action)\n",
    "    newState.marketPrice = price\n",
    "    return newState\n",
    "    \n",
    "        \n",
    "class BitcoinState:\n",
    "    def __init__(self, marketPrice, time, dollarInvestment=1000, coin=0):\n",
    "        self.dollar = dollarInvestment\n",
    "        self.coin = coin\n",
    "        self.marketPrice = marketPrice\n",
    "        time = datetime.fromtimestamp(time).strftime('%Y-%m-%d %H:%M:%S')\n",
    "        self.month = int(time[5:7])\n",
    "        self.day = int(time[8:10])\n",
    "        self.hour = int(time[11:13])\n",
    "        self.minute = int(time[14:16])\n",
    "        \n",
    "    def buyCoin(self, quantity):\n",
    "        self.dollar -= self.marketPrice * quantity\n",
    "        self.coin += quantity\n",
    "        \n",
    "    def sellCoin(self, quantity):\n",
    "        self.dollar += self.marketPrice * quantity\n",
    "        self.coin -= quantity\n",
    "        \n",
    "    def netWorth(self):\n",
    "        return self.dollar + self.coin * self.marketPrice\n",
    "    \n",
    "    def clone(self, marketPrice, time):\n",
    "        return BitcoinState(marketPrice, time, self.dollar, self.coin)\n",
    "            \n",
    "    def isTerminal(self):\n",
    "        return self.netWorth() <= 0\n",
    "        \n",
    "    \n",
    "    \n",
    "def bitcoinFeatureExtractor(state, action):\n",
    "    return [\n",
    "        (\"price\" , state.marketPrice),\n",
    "        (\"month\" , state.month),\n",
    "        (\"day\"   , state.day),\n",
    "        (\"hour\"  , state.hour),\n",
    "        (\"minute\", state.minute),\n",
    "        #(\"dollar\", state.dollar),\n",
    "        (\"coin\"  , state.coin),\n",
    "        (\"coinWorth\", state.coin * state.marketPrice)\n",
    "    ]\n",
    "\n",
    "# Recursively get bucket separating values\n",
    "def bucket_separate(nBuckets,bucket_list,y_distribution):\n",
    "    mid = len(y_distribution)//2 # median index of input y_distribution list\n",
    "    if nBuckets % 2 == 1:\n",
    "        print(\"number of buckets must be 2^n\")\n",
    "\n",
    "    elif nBuckets == 2:\n",
    "        bucket_list.append(y_distribution[mid])\n",
    "\n",
    "    else:\n",
    "        bucket_list.append(y_distribution[mid])\n",
    "        bucket_list = bucket_separate(nBuckets/2,bucket_list,y_distribution[:mid])#left\n",
    "        bucket_list = bucket_separate(nBuckets/2,bucket_list,y_distribution[mid:])#right\n",
    "    bucket_list = sorted(bucket_list)\n",
    "\n",
    "    return bucket_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space = [-10, 0, 10]#[-1000, -100, -10, 0, 10, 100, 1000]\n",
    "#price_distribution = sorted(price)\n",
    "#nBuckets = 1024\n",
    "#buckets = bucket_separate(nBuckets,[-Inf,0,Inf],price_distribution)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myQLearning(qla, numTrials=1000, time_range=60, verbose=True, test = False):\n",
    "    #logging initializations\n",
    "        \n",
    "    totalDiscount = 1\n",
    "    totalRewards = []\n",
    "\n",
    "    for trial in range(numTrials):\n",
    "       \n",
    "        totalReward = 0\n",
    "#         totalReward_percentage = 1\n",
    "        startTimeIndex = random.randint(0, len(price) - 1)\n",
    "        while startTimeIndex + time_range - 1 > len(price) or time[startTimeIndex + time_range] - time[startTimeIndex] != time_range * 60:\n",
    "            startTimeIndex = random.randint(0, len(price) - 1)\n",
    "            #current = switch(price[startTimeIndex],buckets)\n",
    "        \n",
    "        state = BitcoinState(marketPrice=price[startTimeIndex], time=time[startTimeIndex], dollarInvestment=1000, coin=0)\n",
    "        #print(\"Worth:\",state.netWorth(),\"Price:\",price[startTimeIndex], \"Time:\",time[startTimeIndex])\n",
    "        \n",
    "        for i in range(startTimeIndex+1, startTimeIndex+time_range):\n",
    "            #print(\"DEBUG: State=\",state)\n",
    "            #get action based on exploration policy\n",
    "            # ACTION FOR Q LEARNING\n",
    "            \n",
    "            action = qla.getAction(state)     #get a random action\n",
    "            successor = transition(state, action, price=price[i], time=time[i])          #apply action\n",
    "            reward = successor.netWorth() - state.netWorth()        #calculate reward\n",
    "#             reward_percentage = np.log(successor.netWorth() / state.netWorth())\n",
    "            #print(\"Dollar:\",successor.dollar,\"Coin:\",successor.coin,\"Worth:\",successor.netWorth(),\"Price:\",successor.marketPrice, \"Time:\",time[i], \"Action:\",action,\"reward:\",reward)\n",
    "            \n",
    "            #print(\"DEBUG: Action=\",action)\n",
    "            #successor = switch(state+action,buckets)\n",
    "            \n",
    "            #current += 1\n",
    "            #print(\"DEBUG: Succ=\",successor)\n",
    "            \n",
    "            #timer += 1\n",
    "            #terminalState = (timer//60 == 0)\n",
    "\n",
    "            #if not terminalState:\n",
    "            #    reward = price[i] - price[i-1]\n",
    "\n",
    "            totalReward += reward\n",
    "#             totalReward_percentage *= reward_percentage\n",
    "            if successor.isTerminal():\n",
    "                qla.incorporateFeedback(state, action, reward, None)\n",
    "                break;\n",
    "            qla.incorporateFeedback(state, action, reward, successor)\n",
    "\n",
    "            state = successor\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Trial %d totalReward = %s\" % (trial, totalReward))\n",
    "            \n",
    "        totalRewards.append(totalReward)\n",
    "\n",
    "        \n",
    "    return totalRewards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'totalReward_percentage' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-2dee32c5ee16>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-2dee32c5ee16>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m#qla.load()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mtotalRewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmyQLearning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqla\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumTrials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLEARNING_TRIALS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime_range\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0;31m##print(\"Total Rewards:\", totalRewards)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-e1c6b87accf3>\u001b[0m in \u001b[0;36mmyQLearning\u001b[0;34m(qla, numTrials, time_range, verbose, test)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mtotalReward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0mtotalReward_percentage\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mreward_percentage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msuccessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misTerminal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                 \u001b[0mqla\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mincorporateFeedback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'totalReward_percentage' referenced before assignment"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    time_range = 60\n",
    "    LEARNING_TRIALS = 1000000\n",
    "    TESTING_TRIALS = 20\n",
    "    \n",
    "\n",
    "\n",
    "    #Learn with epsilon = 0.5\n",
    "    #qla = QLearningAlgorithm(actions = sawyer.getActions, discount = 1, explorationProb = 0.8)\n",
    "    qla = QLearningApproxAlgorithm(actions = lambda state : action_space, discount = 1, featureExtractor=bitcoinFeatureExtractor, explorationProb = 0.5)\n",
    "    #qla.load()\n",
    "\n",
    "    totalRewards = myQLearning(qla, numTrials=LEARNING_TRIALS, time_range=time_range, test=False)\n",
    "    ##print(\"Total Rewards:\", totalRewards)\n",
    "\n",
    "    print(\"Finish Learning\")\n",
    "\n",
    "    ##Act optimally by setting epsilon = 0\n",
    "    qla.explorationProb = 0\n",
    "    totalRewards = myQLearning(qla, numTrials=TESTING_TRIALS, time_range=time_range,test=True)\n",
    "    print(\"Total Rewards:\", totalRewards)\n",
    "    print(\"Average Rewards:\", sum(totalRewards)/TESTING_TRIALS)\n",
    "    print(\"Finish Testing\")\n",
    "    print(\"Q-Learning Completed\")\n",
    "    #print(\"Number of States explored:\", len(qla.Q))\n",
    "\n",
    "\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
