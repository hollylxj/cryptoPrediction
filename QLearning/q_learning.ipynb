{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from math import pi\n",
    "import time\n",
    "import random\n",
    "from operator import add\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space = [-1, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myQLearning(qla, numTrials=10, maxIterations=1000, verbose=False, test = False):\n",
    "    DOF = len(initial_state)\n",
    "    \n",
    "    totalRewards = []  # The rewards we get on each trial\n",
    "    for trial in range(numTrials):\n",
    "\n",
    "        #sawyer initialization\n",
    "        state = tuple(initial_state)\n",
    "        sawyer.resetPos(initial_state)\n",
    "        sawyer.resetCube()\n",
    "        #raw_input(\"DEBUG: begin trial\")\n",
    "\n",
    "        #logging initializations\n",
    "        sequence = [state]\n",
    "        totalDiscount = 1\n",
    "        totalReward = 0\n",
    "        for count in range(maxIterations):\n",
    "            #print(\"DEBUG: State=\",state)\n",
    "            #get action based on exploration policy\n",
    "            # ACTION FOR Q LEARNING\n",
    "            action = qla.getAction(state)\n",
    "\n",
    "            #print(\"DEBUG: Action=\",action)\n",
    "            successor = tuple(state[i]+action[i] for i in range(DOF))\n",
    "            #print(\"DEBUG: Succ=\",successor)\n",
    "            terminalState = False\n",
    "\n",
    "            #Check for dead angle\n",
    "            for i in range(DOF):\n",
    "                if(successor[i]>jointLimits[i][1] or successor[i]<jointLimits[i][0]):\n",
    "                    reward = reward-100\n",
    "                    terminalState = True\n",
    "                    break\n",
    "\n",
    "            if not terminalState:\n",
    "                if not test:\n",
    "                    if sawyer.moveTo(successor):\n",
    "                        terminalState, reward = sawyer.checkCollision()\n",
    "                    else:\n",
    "                        terminalState, reward = sawyer.checkCollision()\n",
    "                        #reward = reward-100\n",
    "                        \n",
    "                        if reward > -100:\n",
    "                            reward = reward -100\n",
    "                        terminalState = True\n",
    "                else:\n",
    "                    if sawyer.moveTo_test(successor):\n",
    "                        terminalState, reward = sawyer.checkCollision()\n",
    "                    else:\n",
    "                        terminalState, reward = sawyer.checkCollision()\n",
    "                        reward = reward -100\n",
    "                        if reward > -100:\n",
    "                            reward = reward -100\n",
    "                        terminalState = True\n",
    "            \n",
    "            if reward > 0 or reward <=-100:\n",
    "                print(\"reward for trial \", trial,\" out of \", numTrials,\" iteration \", count,\"= \",reward)\n",
    "\n",
    "            totalReward += reward\n",
    "\n",
    "            if terminalState:\n",
    "                qla.incorporateFeedback(state, action, reward, None)\n",
    "                break;\n",
    "            qla.incorporateFeedback(state, action, reward, successor)\n",
    "\n",
    "            state = successor\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Trial %d (totalReward = %s): %s\" % (trial, totalReward, sequence))\n",
    "        totalRewards.append(totalReward)\n",
    "\n",
    "        \n",
    "    return totalRewards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    MAX_ITER = 200\n",
    "    LEARNING_TRIALS = 1000\n",
    "    TESTING_TRIALS = 1\n",
    "    \n",
    "\n",
    "\n",
    "    #Learn with epsilon = 0.5\n",
    "    qla = QLearningAlgorithm(actions = sawyer.getActions, discount = 1, explorationProb = 0.8)\n",
    "    #qla = QLearningApproxAlgorithm(actions = sawyer.getActions, discount = 1, featureExtractor=sawyerFeatureExtractor, explorationProb = 0.5)\n",
    "    #qla.load()\n",
    "\n",
    "    totalRewards = myQLearning(qla, numTrials=LEARNING_TRIALS, maxIterations=MAX_ITER, test=False)\n",
    "    ##print(\"Total Rewards:\", totalRewards)\n",
    "\n",
    "    input(\"Finish Learning\")\n",
    "\n",
    "    ##Act optimally by setting epsilon = 0\n",
    "    qla.explorationProb = 0\n",
    "    totalRewards = myQLearning(qla, numTrials=TESTING_TRIALS, maxIterations=MAX_ITER,test=True)\n",
    "    print(\"Total Rewards:\", totalRewards)\n",
    "    #print(\"Average Rewards:\", sum(totalRewards)/TESTING_TRIALS)\n",
    "    input(\"Finish Testing\")\n",
    "    #print(\"Q-Learning Completed\")\n",
    "    #print(\"Number of States explored:\", len(qla.Q))\n",
    "\n",
    "\n",
    "\n",
    "main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
