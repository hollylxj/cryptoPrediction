{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import random\n",
    "import math\n",
    "import json\n",
    "\n",
    "############################################################\n",
    "\n",
    "# An algorithm that solves an MDP (i.e., computes the optimal\n",
    "# policy).\n",
    "class MDPAlgorithm:\n",
    "    # Set:\n",
    "    # - self.pi: optimal policy (mapping from state to action)\n",
    "    # - self.V: values (mapping from state to best values)\n",
    "    def solve(self, mdp): raise NotImplementedError(\"Override me\")\n",
    "\n",
    "############################################################\n",
    "class ValueIteration(MDPAlgorithm):\n",
    "    '''\n",
    "    Solve the MDP using value iteration.  Your solve() method must set\n",
    "    - self.V to the dictionary mapping states to optimal values\n",
    "    - self.pi to the dictionary mapping states to an optimal action\n",
    "    Note: epsilon is the error tolerance: you should stop value iteration when\n",
    "    all of the values change by less than epsilon.\n",
    "    The ValueIteration class is a subclass of util.MDPAlgorithm (see util.py).\n",
    "    '''\n",
    "    def solve(self, mdp, epsilon=0.001):\n",
    "        mdp.computeStates()\n",
    "        def computeQ(mdp, V, state, action):\n",
    "            # Return Q(state, action) based on V(state).\n",
    "            return sum(prob * (reward + mdp.discount() * V[newState]) \\\n",
    "                            for newState, prob, reward in mdp.succAndProbReward(state, action))\n",
    "\n",
    "        def computeOptimalPolicy(mdp, V):\n",
    "            # Return the optimal policy given the values V.\n",
    "            pi = {}\n",
    "            for state in mdp.states:\n",
    "                pi[state] = max((computeQ(mdp, V, state, action), action) for action in mdp.actions(state))[1]\n",
    "            return pi\n",
    "\n",
    "        V = collections.defaultdict(float)  # state -> value of state\n",
    "        numIters = 0\n",
    "        while True:\n",
    "            newV = {}\n",
    "            for state in mdp.states:\n",
    "                # This evaluates to zero for end states, which have no available actions (by definition)\n",
    "                newV[state] = max(computeQ(mdp, V, state, action) for action in mdp.actions(state))\n",
    "            numIters += 1\n",
    "            if max(abs(V[state] - newV[state]) for state in mdp.states) < epsilon:\n",
    "                V = newV\n",
    "                break\n",
    "            V = newV\n",
    "\n",
    "        # Compute the optimal policy now\n",
    "        pi = computeOptimalPolicy(mdp, V)\n",
    "        print( \"ValueIteration: %d iterations\" % numIters )\n",
    "        self.pi = pi\n",
    "        self.V = V\n",
    "\n",
    "# An abstract class representing a Markov Decision Process (MDP).\n",
    "class MDP:\n",
    "    # Return the start state.\n",
    "    def startState(self): raise NotImplementedError(\"Override me\")\n",
    "\n",
    "    # Return set of actions possible from |state|.\n",
    "    def actions(self, state): raise NotImplementedError(\"Override me\")\n",
    "\n",
    "    # Return a list of (newState, prob, reward) tuples corresponding to edges\n",
    "    # coming out of |state|.\n",
    "    # Mapping to notation from class:\n",
    "    #   state = s, action = a, newState = s', prob = T(s, a, s'), reward = Reward(s, a, s')\n",
    "    # If IsEnd(state), return the empty list.\n",
    "    def succAndProbReward(self, state, action): raise NotImplementedError(\"Override me\")\n",
    "\n",
    "    def discount(self): raise NotImplementedError(\"Override me\")\n",
    "\n",
    "    # Compute set of states reachable from startState.  Helper function for\n",
    "    # MDPAlgorithms to know which states to compute values and policies for.\n",
    "    # This function sets |self.states| to be the set of all states.\n",
    "    def computeStates(self):\n",
    "        self.states = set()\n",
    "        queue = []\n",
    "        self.states.add(self.startState())\n",
    "        queue.append(self.startState())\n",
    "        while len(queue) > 0:\n",
    "            state = queue.pop()\n",
    "            for action in self.actions(state):\n",
    "                for newState, prob, reward in self.succAndProbReward(state, action):\n",
    "                    if newState not in self.states:\n",
    "                        self.states.add(newState)\n",
    "                        queue.append(newState)\n",
    "        # print( \"%d states\" % len(self.states) )\n",
    "        # print( self.states )\n",
    "\n",
    "############################################################\n",
    "\n",
    "# A simple example of an MDP where states are integers in [-n, +n].\n",
    "# and actions involve moving left and right by one position.\n",
    "# We get rewarded for going to the right.\n",
    "class NumberLineMDP(MDP):\n",
    "    def __init__(self, n=5): self.n = n\n",
    "    def startState(self): return 0\n",
    "    def actions(self, state): return [-1, +1]\n",
    "    def succAndProbReward(self, state, action):\n",
    "        return [(state, 0.4, 0),\n",
    "                (min(max(state + action, -self.n), +self.n), 0.6, state)]\n",
    "    def discount(self): return 0.9\n",
    "\n",
    "############################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "############################################################\n",
    "\n",
    "# Abstract class: an RLAlgorithm performs reinforcement learning.  All it needs\n",
    "# to know is the set of available actions to take.  The simulator (see\n",
    "# simulate()) will call getAction() to get an action, perform the action, and\n",
    "# then provide feedback (via incorporateFeedback()) to the RL algorithm, so it can adjust\n",
    "# its parameters.\n",
    "class RLAlgorithm:\n",
    "    # Your algorithm will be asked to produce an action given a state.\n",
    "    def getAction(self, state): raise NotImplementedError(\"Override me\")\n",
    "\n",
    "    # We will call this function when simulating an MDP, and you should update\n",
    "    # parameters.\n",
    "    # If |state| is a terminal state, this function will be called with (s, a,\n",
    "    # 0, None). When this function is called, it indicates that taking action\n",
    "    # |action| in state |state| resulted in reward |reward| and a transition to state\n",
    "    # |newState|.\n",
    "    def incorporateFeedback(self, state, action, reward, newState): raise NotImplementedError(\"Override me\")\n",
    "\n",
    "# An RL algorithm that acts according to a fixed policy |pi| and doesn't\n",
    "# actually do any learning.\n",
    "class FixedRLAlgorithm(RLAlgorithm):\n",
    "    def __init__(self, pi): self.pi = pi\n",
    "\n",
    "    # Just return the action given by the policy.\n",
    "    def getAction(self, state): return self.pi[state]\n",
    "\n",
    "    # Don't do anything: just stare off into space.\n",
    "    def incorporateFeedback(self, state, action, reward, newState): pass\n",
    "\n",
    "# Performs Q-learning.  Read rla.RLAlgorithm for more information.\n",
    "# actions: a function that takes a state and returns a list of actions.\n",
    "# discount: a number between 0 and 1, which determines the discount factor\n",
    "# featureExtractor: a function that takes a state and action and returns a list of (feature name, feature value) pairs.\n",
    "# explorationProb: the epsilon value indicating how frequently the policy\n",
    "# returns a random action\n",
    "class QLearningAlgorithm(RLAlgorithm):\n",
    "    def __init__(self, actions, discount, explorationProb=0.2):\n",
    "        self.actions = actions\n",
    "        self.discount = discount\n",
    "        self.explorationProb = explorationProb\n",
    "        self.numIters = 0\n",
    "        self.Q = {}\n",
    "        random.seed(0)\n",
    "    \n",
    "    def load(self, file=None):\n",
    "        if file is None:\n",
    "            file = \"qvalues.txt\"\n",
    "        self.Q = json.load(open(file))\n",
    "\n",
    "    def save(self, file=None):\n",
    "        if file is None:\n",
    "            file = \"qvalues.txt\"\n",
    "        json.dump(self.Q, open(file,'w'))\n",
    "    \n",
    "    # Return the Q function associated with the weights and features\n",
    "    def getQ(self, state, action):\n",
    "        \n",
    "        state = str(state)\n",
    "        action = str(action)\n",
    "\n",
    "        if state not in self.Q:\n",
    "            self.Q[state] = {}\n",
    "        if action not in self.Q[state]:\n",
    "            self.Q[state][action] = 0.0\n",
    "        return self.Q[state][action]\n",
    "    \n",
    "    # This algorithm will produce an action given a state.\n",
    "    # Here we use the epsilon-greedy algorithm: with probability\n",
    "    # |explorationProb|, take a random action.\n",
    "    def getAction(self, state):\n",
    "        self.numIters += 1\n",
    "        \n",
    "        if random.random() < self.explorationProb:\n",
    "            return random.choice(self.actions(state))\n",
    "        else:\n",
    "            return max((self.getQ(state, action), action) for action in self.actions(state))[1]\n",
    "\n",
    "    # Call this function to get the step size to update the weights.\n",
    "    def getStepSize(self):\n",
    "        #return 1.0 / math.sqrt(self.numIters)\n",
    "        return 0.5\n",
    "        \n",
    "    # We will call this function with (s, a, r, s'), which you should use to update |weights|.\n",
    "    # Note that if s is a terminal state, then s' will be None.  Remember to check for this.\n",
    "    # You should update the weights using self.getStepSize(); use\n",
    "    # self.getQ() to compute the current estimate of the parameters.\n",
    "    def incorporateFeedback(self, state, action, reward, newState):\n",
    "        \n",
    "        prediction = self.getQ(state, action)\n",
    "        \n",
    "        #if state is terminal state, newState would be None\n",
    "        if newState is None:\n",
    "            Vopt = 0\n",
    "        else:\n",
    "            Vopt = max( self.getQ(newState, a) for a in self.actions(newState) )\n",
    "        \n",
    "        target = reward + self.discount * Vopt\n",
    "        eta = self.getStepSize()\n",
    "        self.Q[str(state)][str(action)] = (1-eta)*prediction + eta*target\n",
    "\n",
    "        #if reward > 1:\n",
    "        #   print(\"reward = \", reward)\n",
    "        #   print(\"Vopt = \", Vopt)\n",
    "        #   print(\"Target = \", target)\n",
    "        #   print(\"prediction = \", prediction)\n",
    "        #   print(\"new Q = \", self.Q[str(state)][str(action)])\n",
    "\n",
    "# Performs Q-learning with Approximation.  Read rla.RLAlgorithm for more information.\n",
    "# actions: a function that takes a state and returns a list of actions.\n",
    "# discount: a number between 0 and 1, which determines the discount factor\n",
    "# featureExtractor: a function that takes a state and action and returns a list of (feature name, feature value) pairs.\n",
    "# explorationProb: the epsilon value indicating how frequently the policy\n",
    "# returns a random action\n",
    "class QLearningApproxAlgorithm(RLAlgorithm):\n",
    "    def __init__(self, actions, discount, featureExtractor, explorationProb=0.2):\n",
    "        self.actions = actions\n",
    "        self.discount = discount\n",
    "        self.featureExtractor = featureExtractor\n",
    "        self.explorationProb = explorationProb\n",
    "        self.weights = collections.defaultdict(float)\n",
    "        self.numIters = 0\n",
    "        random.seed(0)\n",
    "    \n",
    "    def load(self, file=None):\n",
    "        if file is None:\n",
    "            file = \"weights.txt\"\n",
    "        self.weights = json.load(open(file))\n",
    "    \n",
    "    def save(self, file=None):\n",
    "        if file is None:\n",
    "            file = \"weights.txt\"\n",
    "        json.dump(self.weights, open(file,'w'))\n",
    "    \n",
    "    # Return the Q function associated with the weights and features\n",
    "    def getQ(self, state, action):\n",
    "        score = 0\n",
    "        for f, v in self.featureExtractor(state, action):\n",
    "            score += self.weights[f] * v\n",
    "        return score\n",
    "    \n",
    "    # This algorithm will produce an action given a state.\n",
    "    # Here we use the epsilon-greedy algorithm: with probability\n",
    "    # |explorationProb|, take a random action.\n",
    "    def getAction(self, state):\n",
    "        self.numIters += 1\n",
    "        if random.random() < self.explorationProb:\n",
    "            return random.choice(self.actions(state))\n",
    "        else:\n",
    "            return max((self.getQ(state, action), action) for action in self.actions(state))[1]\n",
    "\n",
    "    # Call this function to get the step size to update the weights.\n",
    "    def getStepSize(self):\n",
    "        #return 1.0 / math.sqrt(self.numIters)\n",
    "        return 0.2\n",
    "    \n",
    "    # We will call this function with (s, a, r, s'), which you should use to update |weights|.\n",
    "    # Note that if s is a terminal state, then s' will be None.  Remember to check for this.\n",
    "    # You should update the weights using self.getStepSize(); use\n",
    "    # self.getQ() to compute the current estimate of the parameters.\n",
    "    def incorporateFeedback(self, state, action, reward, newState):\n",
    "        \n",
    "        prediction = self.getQ(state, action)\n",
    "        \n",
    "        #if state is terminal state, newState would be None\n",
    "        if newState is None:\n",
    "            Vopt = 0\n",
    "        else:\n",
    "            Vopt = max( self.getQ(newState, a) for a in self.actions(newState) )\n",
    "        \n",
    "        target = reward + self.discount * Vopt\n",
    "        difference = prediction - target\n",
    "        eta = self.getStepSize()\n",
    "        eta_diff = eta * difference\n",
    "        if difference > 0:\n",
    "            print(\"s,a,r,s'=\",state, action, reward, newState)\n",
    "            print(\"Prediction=\",prediction)\n",
    "            print(\"target=\",target)\n",
    "            print(\"difference=\",difference)\n",
    "            print(\"weights=\", self.weights)\n",
    "            #input(\"Hit to update weights\")\n",
    "        for f, v in self.featureExtractor(state, action):\n",
    "            if difference > 0:\n",
    "                print(\"eta*diff*v=\",eta_diff * v)\n",
    "            self.weights[f] -= eta_diff * v\n",
    "        if difference > 0:\n",
    "            print(\"weights=\", self.weights)\n",
    "            #input(\"Weights updated\")\n",
    "\n",
    "# Performs Q-Lambda learning.  Read rla.RLAlgorithm for more information.\n",
    "# actions: a function that takes a state and returns a list of actions.\n",
    "# discount: a number between 0 and 1, which determines the discount factor\n",
    "# featureExtractor: a function that takes a state and action and returns a list of (feature name, feature value) pairs.\n",
    "# explorationProb: the epsilon value indicating how frequently the policy\n",
    "# returns a random action\n",
    "class QLambdaLearningAlgorithm(RLAlgorithm):\n",
    "    def __init__(self, actions, discount, decay, explorationProb=0.2):\n",
    "        self.actions = actions\n",
    "        self.discount = discount\n",
    "        self.explorationProb = explorationProb\n",
    "        self.numIters = 0\n",
    "        self.Q = {}\n",
    "        self.N = {}\n",
    "        self.decay = decay\n",
    "        random.seed(0)\n",
    "    \n",
    "    def load(self, file=None):\n",
    "        if file is None:\n",
    "            file = \"qlambdavalues.txt\"\n",
    "        self.Q = json.load(open(file))\n",
    "    \n",
    "    def save(self, file=None):\n",
    "        if file is None:\n",
    "            file = \"qlambdavalues.txt\"\n",
    "        json.dump(self.Q, open(file,'w'))\n",
    "    \n",
    "    # Return the Q function associated with the weights and features\n",
    "    def getQ(self, state, action):\n",
    "        \n",
    "        state = str(state)\n",
    "        action = str(action)\n",
    "        \n",
    "        if state not in self.Q:\n",
    "            self.Q[state] = {}\n",
    "        if action not in self.Q[state]:\n",
    "            self.Q[state][action] = 0.0\n",
    "        return self.Q[state][action]\n",
    "    \n",
    "    # This algorithm will produce an action given a state.\n",
    "    # Here we use the epsilon-greedy algorithm: with probability\n",
    "    # |explorationProb|, take a random action.\n",
    "    def getAction(self, state):\n",
    "        self.numIters += 1\n",
    "        if random.random() < self.explorationProb:\n",
    "            return random.choice(self.actions(state))\n",
    "        else:\n",
    "            return max((self.getQ(state, action), action) for action in self.actions(state))[1]\n",
    "\n",
    "    # Call this function to get the step size to update the weights.\n",
    "    def getStepSize(self):\n",
    "        #return 1.0 / math.sqrt(self.numIters)\n",
    "        return 0.5\n",
    "    \n",
    "    # We will call this function with (s, a, r, s'), which you should use to update |weights|.\n",
    "    # Note that if s is a terminal state, then s' will be None.  Remember to check for this.\n",
    "    # You should update the weights using self.getStepSize(); use\n",
    "    # self.getQ() to compute the current estimate of the parameters.\n",
    "    def incorporateFeedback(self, state, action, reward, newState):\n",
    "        \n",
    "        prediction = self.getQ(state, action)\n",
    "        if state not in self.N:\n",
    "            self.N[state] = {}\n",
    "        if action not in self.N[state]:\n",
    "            self.N[state][action] = 1.0\n",
    "        else:\n",
    "            self.N[state][action] += 1\n",
    "        \n",
    "        #if state is terminal state, newState would be None\n",
    "        if newState is None:\n",
    "            Vopt = 0\n",
    "        else:\n",
    "            Vopt = max( self.getQ(newState, a) for a in self.actions(newState) )\n",
    "        \n",
    "        target = reward + self.discount * Vopt\n",
    "        diff = target - prediction\n",
    "        eta = self.getStepSize()\n",
    "        #if reward > 0:\n",
    "        #if True:\n",
    "        for nstate in self.N:\n",
    "            for naction in self.N[nstate]:\n",
    "                self.Q[str(nstate)][str(naction)] += eta*diff*self.N[nstate][naction]\n",
    "                self.N[nstate][naction] *= self.discount*self.decay\n",
    "\n",
    "\n",
    "        if newState is None:\n",
    "            self.N = {}\n",
    "\n",
    "############################################################\n",
    "\n",
    "# Perform |numTrials| of the following:\n",
    "# On each trial, take the MDP |mdp| and an RLAlgorithm |rl| and simulates the\n",
    "# RL algorithm according to the dynamics of the MDP.\n",
    "# Each trial will run for at most |maxIterations|.\n",
    "# Return the list of rewards that we get for each trial.\n",
    "def simulate(mdp, rl, numTrials=10, maxIterations=1000, verbose=False,\n",
    "             sort=False):\n",
    "    random.seed(2)\n",
    "    # Return i in [0, ..., len(probs)-1] with probability probs[i].\n",
    "    def sample(probs):\n",
    "        target = random.random()\n",
    "        accum = 0\n",
    "        for i, prob in enumerate(probs):\n",
    "            accum += prob\n",
    "            if accum >= target: return i\n",
    "        raise Exception(\"Invalid probs: %s\" % probs)\n",
    "\n",
    "    totalRewards = []  # The rewards we get on each trial\n",
    "    for trial in range(numTrials):\n",
    "        state = mdp.startState()\n",
    "        sequence = [state]\n",
    "        totalDiscount = 1\n",
    "        totalReward = 0\n",
    "        for _ in range(maxIterations):\n",
    "            action = rl.getAction(state)\n",
    "            transitions = mdp.succAndProbReward(state, action)\n",
    "            if sort: transitions = sorted(transitions)\n",
    "            if len(transitions) == 0:\n",
    "                rl.incorporateFeedback(state, action, 0, None)\n",
    "                break\n",
    "\n",
    "            # Choose a random transition\n",
    "            i = sample([prob for newState, prob, reward in transitions])\n",
    "            newState, prob, reward = transitions[i]\n",
    "            sequence.append(action)\n",
    "            sequence.append(reward)\n",
    "            sequence.append(newState)\n",
    "\n",
    "            rl.incorporateFeedback(state, action, reward, newState)\n",
    "            totalReward += totalDiscount * reward\n",
    "            totalDiscount *= mdp.discount()\n",
    "            state = newState\n",
    "        if verbose:\n",
    "            print( \"Trial %d (totalReward = %s): %s\" % (trial, totalReward, sequence) )\n",
    "        totalRewards.append(totalReward)\n",
    "    return totalRewards\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
